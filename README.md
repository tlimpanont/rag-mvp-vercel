s # RAG MVP - Retrieval-Augmented Generation System# RAG MVP - Retrieval-Augmented Generation System

A production-ready **Minimum Viable Product (MVP)** for a RAG (Retrieval-Augmented Generation) system built with **TypeScript**, **Next.js**, and **Vercel Serverless Functions**.A production-ready **Minimum Viable Product (MVP)** for a RAG (Retrieval-Augmented Generation) system built with **TypeScript**, **Next.js**, and **Vercel Serverless Functions**.A production-ready **Minimum Viable Product (MVP)** for a RAG (Retrieval-Augmented Generation) system built with **TypeScript**, **Next.js**, and **Vercel Serverless Functions**.

## ğŸš€ What is RAG?## ğŸ”§ What is RAG?## ğŸ“– What is RAG?

**Retrieval-Augmented Generation** combines large language models with external knowledge retrieval to provide accurate, up-to-date, and contextually relevant responses based on your specific documents and data.**Retrieval-Augmented Generation** combines large language models with external knowledge retrieval to provide accurate, up-to-date, and contextually relevant responses based on your specific documents and data.**Retrieval-Augmented Generation (RAG)** is a cutting-edge AI architecture that combines the power of large language models with external knowledge retrieval. Instead of relying solely on the model's training data, RAG systems dynamically fetch relevant information from a knowledge base to provide more accurate, up-to-date, and contextually relevant responses.

## âœ¨ Key Features## âœ¨ Key Features### How RAG Works

- âœ… **TypeScript & Next.js** - Modern, type-safe development- âœ… **TypeScript & Next.js** - Modern, type-safe development1. **Document Ingestion**: Your documents are processed, chunked, and converted into vector embeddings

- âœ… **Serverless Architecture** - Scalable Vercel deployment

- âœ… **Vector Search** - Pinecone integration for semantic search- âœ… **Serverless Architecture** - Scalable Vercel deployment2. **Knowledge Storage**: These embeddings are stored in a vector database for fast semantic search

- âœ… **OpenAI Integration** - GPT-4 for intelligent responses

- âœ… **API Security** - Built-in authentication and validation- âœ… **Vector Search** - Pinecone integration for semantic search3. **Query Processing**: When a user asks a question, it's converted into an embedding

- âœ… **Dual Storage** - Postgres + Blob storage support

- âœ… **OpenAI Integration** - GPT-4 for intelligent responses4. **Semantic Search**: The system finds the most relevant document chunks using vector similarity

## ğŸ“ Project Structure

- âœ… **API Security** - Built-in authentication and validation5. **Response Generation**: The retrieved context is combined with the query and sent to an LLM for answer generation

`````

rag-mvp-vercel/- âœ… **Dual Storage** - Postgres + Blob storage support6. **Contextual Response**: The LLM generates an accurate response based on your specific knowledge base

â”œâ”€â”€ app/api/

â”‚   â”œâ”€â”€ rag/query/route.ts       # Main RAG query endpoint## ğŸ“ Project Structure### Key Advantages of RAG

â”‚   â””â”€â”€ ingestion/worker/route.ts # Document processing worker

â”œâ”€â”€ lib/````- **ğŸ¯ Accuracy**: Responses are grounded in your specific documents and data

â”‚   â”œâ”€â”€ clients.ts               # OpenAI & Pinecone clients

â”‚   â””â”€â”€ utils.ts                 # Core utility functionsrag-mvp-vercel/- **ğŸ“ˆ Up-to-date**: Information stays current as new documents are added

â”œâ”€â”€ types/index.ts               # TypeScript definitions

â”œâ”€â”€ docs/                        # Detailed documentationâ”œâ”€â”€ app/api/- **ğŸ” Transparency**: Users can see which sources were used for each answer

â””â”€â”€ db/schema.sql               # Database schema

```â”‚   â”œâ”€â”€ rag/query/route.ts       # Main RAG query endpoint- **ğŸ’° Cost-effective**: No need to fine-tune expensive language models



## ğŸš€ Quick Startâ”‚   â””â”€â”€ ingestion/worker/route.ts # Document processing worker- **ğŸ”’ Privacy**: Your data stays within your infrastructure



### 1. Clone and Installâ”œâ”€â”€ lib/- **ğŸ›ï¸ Control**: Full control over the knowledge base and retrieval process



```bashâ”‚   â”œâ”€â”€ clients.ts               # OpenAI & Pinecone clients

git clone <your-repository>

cd rag-mvp-vercelâ”‚   â””â”€â”€ utils.ts                 # Core utility functions## ğŸ¯ Practical Use Cases

npm install

```â”œâ”€â”€ types/index.ts               # TypeScript definitions



### 2. Environment Setupâ”œâ”€â”€ docs/                        # Detailed documentation### ğŸ“š **Internal Knowledge Management**



```bashâ””â”€â”€ db/schema.sql               # Database schema

cp .env.example .env.local

# Fill in your API keys (see Setup Guide for details)```- **Employee Onboarding**: New hires can ask questions about company policies, procedures, and culture

`````

- **Technical Documentation**: Developers can query API docs, architecture decisions, and coding standards

### 3. Run Locally

## ğŸš€ Quick Start- **HR Support**: Automated responses to common HR questions about benefits, leave policies, and procedures

```bash

npm run dev

```

### 1. Clone and Install### ğŸ›’ **Customer Support & E-commerce**

Visit http://localhost:3000

### 4. Deploy to Vercel

````bash- **Product Information**: Customers can ask detailed questions about product specifications and features

```bash

vercel deploygit clone <your-repository>- **Troubleshooting**: Automated technical support based on manuals and knowledge bases

````

cd rag-mvp-vercel- **Order Support**: Quick answers about shipping, returns, and account information

## ğŸ“š Documentation

npm install

### ğŸ“– **[Setup Guide](./docs/setup.md)**

Complete instructions for obtaining API keys and configuring the system```### ğŸ¥ **Healthcare & Research**

- OpenAI API key setup

- Pinecone configuration

- Vercel deployment

- Environment variables### 2. Environment Setup- **Medical Literature**: Researchers can query vast medical databases and research papers

### ğŸ”§ **[API Documentation](./docs/api.md)**- **Patient Education**: Provide accurate health information based on medical guidelines

Complete API reference and integration examples

- RAG Query Service endpoints```bash- **Clinical Decision Support**: Help healthcare providers with evidence-based recommendations

- Request/response formats

- Testing and integrationcp .env.example .env.local

- Rate limiting and best practices

# Fill in your API keys (see Setup Guide for details)### ğŸ“Š **Financial Services**

### ğŸ’¼ **[Business Case & Use Cases](./docs/business-case.md)**

Business value, ROI analysis, and practical applications````

- Real-world use cases

- ROI calculations and examples- **Compliance**: Quick access to regulatory requirements and compliance procedures

- Implementation timeline

- Success metrics### 3. Run Locally- **Investment Research**: Query financial reports, market analysis, and investment guidelines

### ğŸ—ï¸ **[Technical Documentation](./docs/technical.md)**- **Risk Assessment**: Analyze historical data and risk factors for decision-making

Architecture details and developer guidance

- System architecture```bash

- Code structure and patterns

- Development workflowsnpm run dev### ğŸ“ **Education & Training**

- Troubleshooting guide

````

## ğŸ¯ Common Use Cases

- **Personalized Learning**: Students can ask questions about course materials and textbooks

- **ğŸ“š Internal Knowledge Base** - Employee Q&A and documentation

- **ğŸ›’ Customer Support** - Automated product and service questions  Visit http://localhost:3000- **Research Assistant**: Help with academic research and literature reviews

- **ğŸ¥ Research & Compliance** - Query databases and regulatory docs

- **ğŸ“Š Business Intelligence** - Analyze reports and market data- **Corporate Training**: Employees can query training materials and best practices

- **ğŸ“ Training & Education** - Interactive learning and research

### 4. Deploy to Vercel

## ğŸ“¡ API Example

## ğŸ’¼ Business Case & ROI

```bash

curl -X POST https://your-domain.vercel.app/api/rag/query \```bash

  -H "Content-Type: application/json" \

  -H "Authorization: Bearer your-api-key" \vercel deploy### ğŸ“ˆ **Immediate Business Benefits**

  -d '{

    "query": "What is machine learning?",```

    "maxResults": 5

  }'#### **Cost Reduction (60-80% savings)**

````

## ğŸ“š Documentation

## ğŸ’° Business Value

- **Reduce Support Tickets**: Automate 70% of common customer inquiries

- **60-80% Cost Reduction** in support operations

- **2-4x Productivity Gains** in information access### ğŸ“– **[Setup Guide](./docs/setup.md)**- **Lower Training Costs**: Self-service access to company knowledge reduces onboarding time by 50%

- **15-25% Revenue Growth** through improved customer experience

- **ROI: 1000%+** annually for typical implementationsComplete instructions for obtaining API keys and configuring the system- **Minimize Escalations**: First-level automated responses resolve issues faster

See [Business Case Documentation](./docs/business-case.md) for detailed ROI analysis.- OpenAI API key setup

## ğŸ› ï¸ Development- Pinecone configuration #### **Productivity Gains (2-4x improvement)**

```bash- Vercel deployment

npm run dev          # Start development server

npm run build        # Build for production- Environment variables- **Instant Information Access**: Employees spend 30% less time searching for information

npm run typecheck    # Run TypeScript checks

npm run lint         # Run code linting- **Consistent Answers**: Standardized responses across all departments

```

### ğŸ”§ **[API Documentation](./docs/api.md)**- **24/7 Availability**: No waiting for human experts or business hours

## ğŸ“Š Monitoring

Complete API reference and integration examples

Access function logs and metrics in your [Vercel Dashboard](https://vercel.com/dashboard):

1. Navigate to your project- RAG Query Service endpoints#### **Revenue Growth (15-25% increase)**

2. Click **Functions**

3. Select function to view logs and performance- Request/response formats

## ğŸ” Security- Testing and integration- **Improved Customer Experience**: Faster response times lead to higher satisfaction

- **API Key Authentication** - Secure endpoint access- Rate limiting and best practices- **Sales Enablement**: Sales teams have instant access to product information and competitive intelligence

- **Environment Variables** - Safe credential management

- **Input Validation** - Request sanitization and validation- **Market Expansion**: Support multiple languages and time zones without additional staff

- **Rate Limiting** - Built-in protection against abuse

### ğŸ’¼ **[Business Case & Use Cases](./docs/business-case.md)**

## ğŸ¤ Contributing

Business value, ROI analysis, and practical applications### ğŸš€ **Strategic Advantages**

This MVP is designed to be extended. Common enhancements:

- Additional data sources and integrations- Real-world use cases

- Custom embedding models

- Advanced chunking strategies - ROI calculations and examples#### **Scalability**

- Streaming responses

- Caching layers- Implementation timeline

## ğŸ“„ License- Success metrics- Handle 10,000+ queries simultaneously without linear cost increase

MIT License - see LICENSE file for details- Easy integration with existing systems and workflows

---### ğŸ—ï¸ **[Technical Documentation](./docs/technical.md)**- Rapid deployment across multiple departments or business units

**ğŸš€ Ready to transform your knowledge management?** Architecture details and developer guidance

Start with the [Setup Guide](./docs/setup.md) or explore the [Business Case](./docs/business-case.md) to understand the value proposition.

- System architecture#### **Data-Driven Insights**

**Built with TypeScript, Next.js, and Vercel Serverless** âš¡

- Code structure and patterns

- Development workflows- Track most common questions to identify knowledge gaps

- Troubleshooting guide- Monitor response accuracy and user satisfaction

- Optimize content strategy based on usage patterns

## ğŸ¯ Common Use Cases

#### **Competitive Differentiation**

- **ğŸ“š Internal Knowledge Base** - Employee Q&A and documentation

- **ğŸ›’ Customer Support** - Automated product and service questions - Faster time-to-market with new product information

- **ğŸ¥ Research & Compliance** - Query databases and regulatory docs- Superior customer experience compared to traditional support channels

- **ğŸ“Š Business Intelligence** - Analyze reports and market data- Ability to leverage proprietary knowledge as a competitive advantage

- **ğŸ“ Training & Education** - Interactive learning and research

### ğŸ’° **ROI Example: Mid-size Company (500 employees)**

## ğŸ“¡ API Example

**Investment**: $2,000/month (infrastructure + API costs)

```bash

curl -X POST https://your-domain.vercel.app/api/rag/query \**Returns**:

  -H "Content-Type: application/json" \

  -H "Authorization: Bearer your-api-key" \- **Support Cost Savings**: $8,000/month (4 support agents Ã— $2K salary)

  -d '{- **Productivity Gains**: $12,000/month (200 employees Ã— 2 hours saved Ã— $30/hour)

    "query": "What is machine learning?",- **Training Efficiency**: $3,000/month (50% reduction in onboarding time)

    "maxResults": 5

  }'**Net ROI**: 1,050% annually ($276,000 return on $24,000 investment)

```

### ğŸ¯ **Success Metrics to Track**

## ğŸ” Security

- **Response Accuracy**: >90% user satisfaction with answers

- **API Key Authentication** - Secure endpoint access- **Query Resolution Time**: <30 seconds average response time

- **Environment Variables** - Safe credential management- **Cost per Query**: <$0.10 per interaction

- **Input Validation** - Request sanitization and validation- **Adoption Rate**: >70% employee/customer usage within 6 months

- **Rate Limiting** - Built-in protection against abuse- **Knowledge Coverage**: >95% of common questions answerable

## ğŸ’° Business Value## ğŸ¢ **Implementation Timeline**

- **60-80% Cost Reduction** in support operations### **Week 1-2: Foundation Setup**

- **2-4x Productivity Gains** in information access

- **15-25% Revenue Growth** through improved customer experience- Deploy RAG infrastructure (this MVP)

- **ROI: 1000%+** annually for typical implementations- Set up vector database and LLM connections

- Configure security and access controls

See [Business Case Documentation](./docs/business-case.md) for detailed ROI analysis.

### **Week 3-4: Content Ingestion**

## ğŸ› ï¸ Development

- Upload and process initial document set

````bash- Test query accuracy and response quality

npm run dev          # Start development server- Fine-tune retrieval parameters

npm run build        # Build for production

npm run typecheck    # Run TypeScript checks### **Week 5-6: Integration & Training**

npm run lint         # Run code linting

```- Integrate with existing systems (Slack, Teams, website)

- Train initial user groups

## ğŸ“Š Monitoring- Gather feedback and iterate



Access function logs and metrics in your [Vercel Dashboard](https://vercel.com/dashboard):### **Week 7-8: Scale & Optimize**

1. Navigate to your project

2. Click **Functions** - Roll out to full organization

3. Select function to view logs and performance- Monitor performance and usage patterns

- Continuously improve knowledge base

## ğŸ¤ Contributing

## ğŸ—ï¸ Architecture

This MVP is designed to be extended. Common enhancements:

- Additional data sources and integrationsThis MVP consists of two independent serverless functions:

- Custom embedding models

- Advanced chunking strategies  1. **RAG Query Service** (`/api/rag/query`) - Direct API endpoint for querying the RAG system

- Streaming responses2. **Ingestion Worker** (`/api/ingestion/worker`) - Background task for document processing (Cron-triggered)

- Caching layers

### Key Features

## ğŸ“„ License

- âœ… **Strong TypeScript Typing** - Custom interfaces and types throughout

MIT License - see LICENSE file for details- âœ… **Vercel Optimization** - Global client initialization to minimize cold starts

- âœ… **Security** - Mandatory API key validation for query service

---- âœ… **Modularity** - Clean separation of concerns (API routes, types, utilities)

- âœ… **Vector Search** - Pinecone integration for semantic search

**ğŸš€ Ready to transform your knowledge management?**  - âœ… **LLM Integration** - OpenAI GPT-4 for answer generation

Start with the [Setup Guide](./docs/setup.md) or explore the [Business Case](./docs/business-case.md) to understand the value proposition.- âœ… **Dual Storage** - Vercel Postgres + Blob Storage support



**Built with TypeScript, Next.js, and Vercel Serverless** âš¡## ğŸ“ Project Structure

````

rag-mvp-vercel/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â”œâ”€â”€ rag/
â”‚ â”‚ â”‚ â””â”€â”€ query/
â”‚ â”‚ â”‚ â””â”€â”€ route.ts # RAG Query Service
â”‚ â”‚ â””â”€â”€ ingestion/
â”‚ â”‚ â””â”€â”€ worker/
â”‚ â”‚ â””â”€â”€ route.ts # Ingestion Worker
â”‚ â”œâ”€â”€ layout.tsx
â”‚ â””â”€â”€ page.tsx
â”œâ”€â”€ lib/
â”‚ â”œâ”€â”€ clients.ts # Global service clients (OpenAI, Pinecone)
â”‚ â””â”€â”€ utils.ts # Utility functions
â”œâ”€â”€ types/
â”‚ â””â”€â”€ index.ts # TypeScript type definitions
â”œâ”€â”€ db/
â”‚ â””â”€â”€ schema.sql # PostgreSQL schema
â”œâ”€â”€ .env.example # Environment variables template
â”œâ”€â”€ vercel.json # Vercel configuration + Cron jobs
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ next.config.js
â””â”€â”€ README.md

````

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+
- Vercel account
- OpenAI API key
- Pinecone account and index
- Vercel Postgres database (optional)
- Vercel Blob Storage (optional)

### 1. Clone and Install

```bash
cd /Users/theuylimpanont/Development/rag-mvp-vercel
npm install
````

### 2. Configure Environment Variables

Copy `.env.example` to `.env.local` and fill in your credentials:

```bash
cp .env.example .env.local
```

#### ğŸ” How to Obtain API Keys and Environment Variables

##### OpenAI API Key

1. Visit [OpenAI Platform](https://platform.openai.com/api-keys)
2. Sign up or log into your OpenAI accountw
3. Navigate to **API Keys** section
4. Click **"Create new secret key"**
5. Copy the key (starts with `sk-`) - store it safely as it won't be shown again
6. Set your usage limits and billing in the [Usage Dashboard](https://platform.openai.com/usage)

##### Pinecone Configuration

1. Go to [Pinecone Console](https://app.pinecone.io/)
2. Sign up for a free account (includes 100K vectors)
3. Create a new project or use the default one
4. **API Key**: Navigate to **API Keys** â†’ Copy your API key
5. **Environment**: Note your environment (e.g., `us-east-1-aws`, `us-west1-gcp`)
6. **Index**: Create a new index with these settings:
   - **Name**: `rag-mvp-vercel` (or your preferred name)
   - **Dimensions**: `1536` (for OpenAI's text-embedding-3-small)
   - **Metric**: `cosine`
   - **Pod Type**: `Starter` (free tier) or `s1.x1`

##### Vercel Postgres Database (Optional)

1. Go to [Vercel Dashboard](https://vercel.com/dashboard)
2. Select your project or create a new one
3. Navigate to **Storage** â†’ **Create Database** â†’ **Postgres**
4. Choose your plan (Hobby is free with limitations)
5. After creation, go to **Settings** â†’ **Environment Variables**
6. Copy the three connection strings:
   - `POSTGRES_URL`
   - `POSTGRES_PRISMA_URL`
   - `POSTGRES_URL_NON_POOLING`

##### Vercel Blob Storage (Optional)

1. In [Vercel Dashboard](https://vercel.com/dashboard), go to your project
2. Navigate to **Storage** â†’ **Create Database** â†’ **Blob**
3. Choose your plan (Hobby includes 1GB free)
4. After creation, get your token from **Settings** â†’ **Environment Variables**
5. Copy the `BLOB_READ_WRITE_TOKEN`

##### API Security Keys

Generate secure random keys for:

- **RAG_API_KEY**: Create a strong random string (recommended 32+ characters)

  ```bash
  # Generate with OpenSSL
  openssl rand -base64 32

  # Or use Node.js
  node -e "console.log(require('crypto').randomBytes(32).toString('base64'))"
  ```

- **CRON_SECRET** (Optional): Another secure random string for cron job protection

##### Application Settings

These are pre-configured in `.env.example` but can be customized:

- **EMBEDDING_MODEL**: `text-embedding-3-small` (OpenAI's latest, cost-effective)
- **EMBEDDING_DIMENSIONS**: `1536` (matches the embedding model)
- **LLM_MODEL**: `gpt-4-turbo-preview` or `gpt-4o` (latest models)
- **TOP_K_RESULTS**: `5` (number of documents to retrieve per query)

#### Required Environment Variables

```env
# OpenAI Configuration
OPENAI_API_KEY=sk-...

# Pinecone Configuration
PINECONE_API_KEY=...
PINECONE_ENVIRONMENT=us-east-1-aws
PINECONE_INDEX_NAME=rag-mvp-vercel

# API Security (Generate a strong random key)
RAG_API_KEY=your-secure-api-key-here

# Application Settings
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=1536
LLM_MODEL=gpt-4-turbo-preview
TOP_K_RESULTS=5
```

#### Optional (for Ingestion Worker)

```env
# Vercel Postgres
POSTGRES_URL=postgres://...
POSTGRES_PRISMA_URL=postgres://...
POSTGRES_URL_NON_POOLING=postgres://...

# Vercel Blob Storage
BLOB_READ_WRITE_TOKEN=vercel_blob_...

# Cron Security (Optional)
CRON_SECRET=your-cron-secret
```

### 3. Set Up Database (Optional)

If using Vercel Postgres, run the schema:

```bash
# Connect to your Vercel Postgres instance and execute:
psql $POSTGRES_URL < db/schema.sql
```

### 4. Set Up Pinecone Index

Create a Pinecone index with the following settings:

- **Dimensions**: 1536 (for `text-embedding-3-small`)
- **Metric**: cosine
- **Pod Type**: Starter or higher

### 5. Run Locally

```bash
npm run dev
```

Visit http://localhost:3000 to see the homepage.

### 6. Deploy to Vercel

```bash
vercel deploy
```

Add all environment variables in the Vercel dashboard under **Settings â†’ Environment Variables**.

## ğŸ“¡ API Usage

### RAG Query Service

**Endpoint:** `POST /api/rag/query`

**Authentication:** Required (API Key via `Authorization` header)

#### Request

```bash
curl -X POST https://your-domain.vercel.app/api/rag/query \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-secure-api-key-here" \
  -d '{
    "query": "What is machine learning?",
    "maxResults": 5,
    "includeMetadata": true
  }'
```

#### Request Body

```typescript
{
  query: string;              // Required: User's question
  maxResults?: number;        // Optional: Number of documents to retrieve (default: 5)
  includeMetadata?: boolean;  // Optional: Include document metadata (default: true)
}
```

#### Response

```json
{
  "answer": "Machine learning is a subset of artificial intelligence...",
  "sources": [
    {
      "id": "doc_123_chunk_0",
      "content": "Machine learning is...",
      "score": 0.89,
      "metadata": {
        "title": "Introduction to ML",
        "url": "https://example.com/ml-intro",
        "documentType": "article"
      }
    }
  ],
  "metadata": {
    "queryProcessingTimeMs": 1245,
    "vectorSearchTimeMs": 156,
    "llmGenerationTimeMs": 1089,
    "totalDocumentsSearched": 5,
    "model": "gpt-4-turbo-preview"
  }
}
```

#### Health Check

```bash
curl -X GET https://your-domain.vercel.app/api/rag/query \
  -H "Authorization: Bearer your-secure-api-key-here"
```

### Ingestion Worker

**Endpoint:** `POST /api/ingestion/worker`

**Triggered by:** Vercel Cron Job (every 6 hours)

**Manual Trigger:**

```bash
curl -X POST https://your-domain.vercel.app/api/ingestion/worker \
  -H "Authorization: Bearer your-cron-secret"
```

#### Response

```json
{
  "success": true,
  "jobId": "ingest_1698765432000",
  "documentsProcessed": 15,
  "errors": [],
  "duration": 45230
}
```

## ğŸ”§ Configuration

### Vercel Cron Jobs

Configured in `vercel.json`:

```json
{
  "crons": [
    {
      "path": "/api/ingestion/worker",
      "schedule": "0 */6 * * *" // Every 6 hours
    }
  ]
}
```

Schedule formats:

- `0 */6 * * *` - Every 6 hours
- `0 0 * * *` - Daily at midnight
- `0 */1 * * *` - Every hour

### Function Timeouts

- **RAG Query Service**: 30 seconds (Edge Runtime)
- **Ingestion Worker**: 300 seconds (Node.js Runtime)

## ğŸ—ï¸ Key Components

### 1. Global Service Clients (`lib/clients.ts`)

Singleton pattern for OpenAI and Pinecone clients to minimize cold start times:

```typescript
import {
  getOpenAIClient,
  getPineconeClient,
  getPineconeIndex,
} from "@/lib/clients";
```

### 2. Utility Functions (`lib/utils.ts`)

Core functions for:

- API key validation
- Embedding generation (single & batch)
- Vector search
- Vector upsert
- Text chunking
- LLM response generation

### 3. Type Definitions (`types/index.ts`)

Comprehensive TypeScript interfaces for:

- RAG queries and responses
- Ingestion jobs
- Vector documents
- API responses
- Database records

## ğŸ”’ Security

### API Key Protection

The RAG Query Service requires an API key:

```typescript
// Set in environment
RAG_API_KEY=your-secure-api-key-here

// Client request
Authorization: Bearer your-secure-api-key-here
```

### Cron Job Protection (Optional)

Protect the ingestion worker:

```typescript
// Set in environment
CRON_SECRET = your - cron - secret;

// Vercel Cron automatically includes this in requests
```

## ğŸ§ª Testing

### Test Query Service

```bash
# Using curl
curl -X POST http://localhost:3000/api/rag/query \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer your-api-key" \
  -d '{"query":"Test question"}'

# Using Open WebUI
# Configure the endpoint: http://localhost:3000/api/rag/query
# Add API key in headers
```

### Test Ingestion Worker

```bash
curl -X POST http://localhost:3000/api/ingestion/worker
```

## ğŸ“Š Monitoring

View logs in Vercel Dashboard:

1. Go to your deployment
2. Click **Functions**
3. Select the function to view logs

Key metrics to monitor:

- Query processing time
- Vector search latency
- LLM generation time
- Ingestion job success rate

## ğŸ”„ Data Flow

### Query Pipeline

```
User Query
  â†’ API Key Validation
  â†’ Generate Embedding
  â†’ Vector Search (Pinecone)
  â†’ Retrieve Top K Documents
  â†’ Generate LLM Response (OpenAI)
  â†’ Return Answer + Sources
```

### Ingestion Pipeline

```
Cron Trigger (Every 6h)
  â†’ Fetch Unprocessed Documents
    â”œâ”€â”€ Vercel Blob Storage
    â””â”€â”€ Vercel Postgres
  â†’ Chunk Text
  â†’ Generate Embeddings (Batch)
  â†’ Upsert to Pinecone
  â†’ Mark as Indexed
```

## ğŸ› ï¸ Development

### Type Checking

```bash
npm run typecheck
```

### Build

```bash
npm run build
```

### Lint

```bash
npm run lint
```

## ğŸ“ Integration with Open WebUI

Configure Open WebUI to use this RAG service:

1. Open WebUI Settings â†’ **Knowledge** â†’ **External RAG**
2. Set **Endpoint**: `https://your-domain.vercel.app/api/rag/query`
3. Add **Header**: `Authorization: Bearer your-api-key`
4. Set **Method**: POST
5. Test connection

## ğŸš¨ Troubleshooting

### Cold Start Issues

- Global clients are initialized once per container
- First request may be slower (~500-1000ms)
- Subsequent requests are fast (~100-300ms)

### Vector Search Returns No Results

- Verify Pinecone index has documents
- Check embedding dimensions match (1536 for `text-embedding-3-small`)
- Review query embedding generation

### Ingestion Failures

- Check database connection
- Verify blob storage permissions
- Review function timeout settings (increase if needed)

## ğŸ“„ License

MIT

## ğŸ¤ Contributing

This is an MVP template. Feel free to extend with:

- Additional data sources
- Custom embedding models
- Advanced chunking strategies
- Streaming responses
- Rate limiting
- Caching layers

---

**Built with TypeScript, Next.js, and Vercel Serverless** ğŸš€
